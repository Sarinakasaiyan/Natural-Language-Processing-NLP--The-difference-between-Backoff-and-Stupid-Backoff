{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5eiIEtjctUx",
        "outputId": "cedc4bee-4056-4c2f-aa59-ceb0844f7c5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.16666666666666666\n",
            "0.1111111111111111\n",
            "0.08333333333333333\n"
          ]
        }
      ],
      "source": [
        "#SarinaKasaiyan\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "class BackoffLanguageModel:\n",
        "    def __init__(self, n):\n",
        "        self.n = n  # Maximum order of n-grams (e.g., 1 for unigram, 2 for bigram, etc.)\n",
        "        self.ngrams = defaultdict(Counter)  # Dictionary to hold counts of n-grams\n",
        "\n",
        "    def train(self, sentences):\n",
        "        \"\"\"Train the model using the provided sentences.\"\"\"\n",
        "        for sentence in sentences:\n",
        "            tokens = sentence.split()  # Split the sentence into words (tokens)\n",
        "            for i in range(len(tokens)):\n",
        "                # Generate n-grams of order from 1 to n\n",
        "                for j in range(1, self.n + 1):\n",
        "                    if i + j <= len(tokens):  # Ensure we don't go out of bounds\n",
        "                        ngram = tuple(tokens[i:i + j])  # Create an n-gram as a tuple\n",
        "                        self.ngrams[j][ngram] += 1  # Increment the count of this n-gram\n",
        "\n",
        "    def backoff_probability(self, ngram):\n",
        "        \"\"\"Calculate the probability of an n-gram using backoff.\"\"\"\n",
        "        order = len(ngram)  # Get the order of the n-gram (number of words)\n",
        "\n",
        "        if order > self.n:  # If the order exceeds the maximum, return probability of zero\n",
        "            return 0.0\n",
        "\n",
        "        # Check the count of the current n-gram\n",
        "        count = self.ngrams[order][ngram]\n",
        "\n",
        "        if count > 0:\n",
        "            # If the n-gram exists, return its probability\n",
        "            return count / sum(self.ngrams[order].values())\n",
        "\n",
        "        # Back off to lower order n-grams if current n-gram is not found\n",
        "        for backoff_order in range(order - 1, 0, -1):  # Loop through lower orders\n",
        "            backoff_ngram = ngram[1:]  # Remove the first word to create a backoff n-gram\n",
        "            count = self.ngrams[backoff_order][backoff_ngram]  # Get count for backoff n-gram\n",
        "\n",
        "            if count > 0:\n",
        "                # If the backoff n-gram exists, calculate its probability and apply scaling\n",
        "                return (0.4 * count) / sum(self.ngrams[backoff_order].values())\n",
        "\n",
        "        return 0.0  # If no valid n-gram is found, return zero probability\n",
        "\n",
        "# Example usage\n",
        "sentences = [\n",
        "    \"I want to go\",\n",
        "    \"I want to eat\",\n",
        "    \"I want to sleep\"\n",
        "]\n",
        "\n",
        "# Create an instance of BackoffLanguageModel with maximum n=3 (trigrams)\n",
        "model = BackoffLanguageModel(n=3)\n",
        "model.train(sentences)  # Train the model with example sentences\n",
        "\n",
        "# Print probabilities for various n-grams\n",
        "print(model.backoff_probability((\"want\", \"to\", \"go\")))  # Probability of trigram\n",
        "print(model.backoff_probability((\"to\", \"go\")))           # Probability of bigram\n",
        "print(model.backoff_probability((\"go\",)))                 # Probability of unigram"
      ]
    }
  ]
}
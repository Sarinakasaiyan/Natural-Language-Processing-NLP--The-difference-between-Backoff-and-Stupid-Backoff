{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BTisZqJczC3",
        "outputId": "0579f57a-8c86-46ca-a3d5-7324ac16372a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stupid Backoff Probabilities:\n",
            "0.16666666666666666\n",
            "0.1111111111111111\n",
            "0.08333333333333333\n"
          ]
        }
      ],
      "source": [
        "#SarinaKasaiyan\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "class StupidBackoffLanguageModel:\n",
        "    def __init__(self, n):\n",
        "        self.n = n  # Maximum n-gram order (e.g., 1 for unigram, 2 for bigram, etc.)\n",
        "        self.ngrams = defaultdict(Counter)  # Dictionary to hold counts of n-grams\n",
        "\n",
        "    def train(self, sentences):\n",
        "        \"\"\"Train the model using the provided sentences.\"\"\"\n",
        "        for sentence in sentences:\n",
        "            tokens = sentence.split()  # Split the sentence into words (tokens)\n",
        "            for i in range(len(tokens)):\n",
        "                # Generate n-grams of order from 1 to n\n",
        "                for j in range(1, self.n + 1):\n",
        "                    if i + j <= len(tokens):  # Ensure we don't go out of bounds\n",
        "                        ngram = tuple(tokens[i:i + j])  # Create an n-gram as a tuple\n",
        "                        self.ngrams[j][ngram] += 1  # Increment the count of this n-gram\n",
        "\n",
        "    def stupid_backoff_probability(self, ngram):\n",
        "        \"\"\"Calculate the probability of an n-gram using Stupid Backoff.\"\"\"\n",
        "        order = len(ngram)  # Get the order of the n-gram (number of words)\n",
        "\n",
        "        # Check the count of the current n-gram\n",
        "        count = self.ngrams[order][ngram]\n",
        "\n",
        "        if count > 0:\n",
        "            # If the n-gram exists, return its probability\n",
        "            return count / sum(self.ngrams[order].values())\n",
        "\n",
        "        # Back off to lower order n-grams with a fixed scaling factor\n",
        "        scaling_factor = 0.4\n",
        "\n",
        "        for backoff_order in range(order - 1, 0, -1):  # Loop through lower orders\n",
        "            backoff_ngram = ngram[1:]  # Remove the first word to create a backoff n-gram\n",
        "            count = self.ngrams[backoff_order][backoff_ngram]  # Get count for backoff n-gram\n",
        "\n",
        "            if count > 0:\n",
        "                # If the backoff n-gram exists, calculate its probability and apply scaling\n",
        "                return scaling_factor * (count / sum(self.ngrams[backoff_order].values()))\n",
        "\n",
        "        # Finally, fall back to unigram probability if all else fails\n",
        "        return scaling_factor * (self.ngrams[1][ngram[-1]] / sum(self.ngrams[1].values()))\n",
        "\n",
        "# Example usage\n",
        "sentences = [\n",
        "    \"I want to go\",\n",
        "    \"I want to eat\",\n",
        "    \"I want to sleep\"\n",
        "]\n",
        "\n",
        "# Create an instance of Stupid Backoff model with maximum n=3 (trigrams)\n",
        "stupid_model = StupidBackoffLanguageModel(n=3)\n",
        "stupid_model.train(sentences)  # Train the model with example sentences\n",
        "\n",
        "# Print probabilities for various n-grams\n",
        "print(\"Stupid Backoff Probabilities:\")\n",
        "print(stupid_model.stupid_backoff_probability((\"want\", \"to\", \"go\")))  # Trigram probability\n",
        "print(stupid_model.stupid_backoff_probability((\"to\", \"go\")))           # Bigram probability\n",
        "print(stupid_model.stupid_backoff_probability((\"go\",)))                 # Unigram probability"
      ]
    }
  ]
}